from copy import deepcopy
from typing import Dict, Union

import numpy as np
import pandas as pd

from gluonts.dataset.pandas import PandasDataset

# from gluonts.model.simple_feedforward import SimpleFeedForwardEstimator
from gluonts.model.deepar import DeepAREstimator
# from gluonts.model.renewal import DeepRenewalProcessEstimator
from gluonts.mx import Trainer

from lightwood.helpers.log import log
from lightwood.mixer.base import BaseMixer
from lightwood.api.types import PredictionArguments
from lightwood.data.encoded_ds import EncodedDs, ConcatedEncodedDs


class GluonTSMixer(BaseMixer):
    horizon: int
    target: str
    supports_proba: bool
    model_path: str
    hyperparam_search: bool
    default_config: dict

    def __init__(
            self,
            stop_after: float,
            target: str,
            horizon: int,
            window: int,
            dtype_dict: Dict,
            ts_analysis: Dict,
    ):
        """
        Wrapper around GlounTS probabilistic deep learning models. For now, only DeepAR is supported.

        :param stop_after: time budget in seconds.
        :param target: column to forecast.
        :param horizon: length of forecasted horizon.
        :param window: length of input data.
        :param ts_analysis: dictionary with miscellaneous time series info, as generated by 'lightwood.data.timeseries_analyzer'.
        """  # noqa
        super().__init__(stop_after)
        self.stable = True
        self.prepared = False
        self.supports_proba = True  # ?
        self.target = target
        self.window = window
        self.horizon = horizon
        self.n_epochs = 10
        self.dtype_dict = dtype_dict
        self.ts_analysis = ts_analysis
        self.grouped_by = ['__default'] if not ts_analysis['tss'].group_by else ts_analysis['tss'].group_by
        self.model = None
        self.train_cache = None

    def fit(self, train_data: EncodedDs, dev_data: EncodedDs) -> None:
        """ Fits the model. """  # noqa
        log.info('Started fitting GluonTS forecasting model')

        # prepare data
        cat_ds = ConcatedEncodedDs([train_data, dev_data])
        train_ds = self._make_initial_ds(cat_ds.data_frame, train=True)

        estimator = DeepAREstimator(
            freq=train_ds.freq,
            prediction_length=self.horizon,
            trainer=Trainer(epochs=self.n_epochs)
        )
        self.model = estimator.train(train_ds)
        log.info('Successfully trained GluonTS forecasting model.')

    def partial_fit(self, train_data: EncodedDs, dev_data: EncodedDs) -> None:
        """
        Due to how lightwood implements the `update` procedure, expected inputs for this method are:

        :param dev_data: original `test` split (used to validate and select model if ensemble is `BestOf`).
        :param train_data: concatenated original `train` and `dev` splits.
        """  # noqa
        self.hyperparam_search = False
        self.fit(dev_data, train_data)
        self.prepared = True

    def __call__(self, ds: Union[EncodedDs, ConcatedEncodedDs],
                 args: PredictionArguments = PredictionArguments()) -> pd.DataFrame:
        """ 
        Calls the mixer to emit forecasts.
        """  # noqa
        length = sum(ds.encoded_ds_lenghts) if isinstance(ds, ConcatedEncodedDs) else len(ds)
        ydf = pd.DataFrame(0,  # zero-filled
                           index=np.arange(length),
                           columns=['prediction', 'lower', 'upper'],
                           dtype=object)
        ydf['index'] = ds.data_frame.index
        conf = args.fixed_confidence if args.fixed_confidence else 0.9
        ydf['confidence'] = conf

        for idx in range(length):
            df = ds.data_frame.iloc[:idx] if idx != 0 else None
            input_ds = self._make_initial_ds(df)
            forecasts = list(self.model.predict(input_ds))[0]
            ydf.at[idx, 'prediction'] = [entry for entry in forecasts.mean]
            ydf.at[idx, 'lower'] = [entry for entry in forecasts.quantile(1 - conf)]
            ydf.at[idx, 'upper'] = [entry for entry in forecasts.quantile(conf)]

        return ydf

    def _make_initial_ds(self, df=None, train=False):
        oby = self.ts_analysis["tss"].order_by
        gby = self.ts_analysis["tss"].group_by if self.ts_analysis["tss"].group_by else []
        freq = self.ts_analysis['sample_freqs']['__default']
        keep_cols = [f'__mdb_original_{oby}', self.target] + [col for col in gby]

        if df is None and not train:
            df = self.train_cache
        else:
            sub_df = df[keep_cols]
            df = deepcopy(sub_df)

            if train:
                self.train_cache = df
            else:
                df = pd.concat([self.train_cache, df]).sort_index()

        if gby:
            # TODO: multiple group support
            invalid_groups = []
            for g in df[gby[0]].unique():
                if len(df[df[gby[0]] == g]) < self.horizon:
                    invalid_groups.append(g)
            df = df[~df[gby[0]].isin(invalid_groups)]
        else:
            gby = '__default_group'
            df[gby] = '__default_group'

        ds = PandasDataset.from_long_dataframe(df, target=self.target, item_id=gby, freq=freq)
        return ds
